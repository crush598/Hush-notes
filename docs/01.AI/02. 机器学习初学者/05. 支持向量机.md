---
title:  支持向量机
date: 2022-10-21 16:55:00
permalink: /pages/f64165/
tags:
  - 机器学习
---

# 支持向量机

​    支持向量机（Support Vector Machine, SVM）是一种二分类的监督学习方法，SVM 的目标是通过间隔最大化求解最优的超平面。

## 硬间隔支持向量机

​	一般地，当训练数据线性可分时，存在无数个超平面可将两类数据正确分开。SVM 期望找到一个能够正确划分训练数据集并且「几何间隔」最大的分离超平面。

<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI202210302258318.png" alt="iShot_2022-10-30_19.58.16" style="zoom:50%;" />

决策边界就是 $f(x)$ ，红色和蓝色的线是支持向量（support vector）所在的面，红色、蓝色线之间的间隙就是我们要最大化的分类间的间隙 M（Margin Width)。

> - 间隔：$r = \frac{2}{||w||}$
>
> - 几何间隔
>
>     定义超平面(w,b)关于样本点(xi,yi)的**几何间隔**：
>
>     ​	$r_i = \frac{y_i(w^Tx_i + b)}{||w||}$
>
> - 支持向量
>
>     在线性可分的情况下，训练数据集的样本点中与分离超平面距离最近的数据点称为支持向量(support vector)，即所有在直线 $w^Tx +b = 1$ 或直线 $w^Tx + b = -1$ 的点。
>
> - **在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用**

-  ==**根据以上定义 SVM 模型可以表示为带约束的最优化问题：**$$\max\limits_{w,b} \gamma \\ s.t. \frac{y_i(w^tx_i + b)}{||w||} \geq \frac{\gamma}{2},i=1,2,...,N$$== 
-  当 $\gamma = \frac{2}{||w||}$ 时，上式等价于$$\max\limits_{w,b} \frac{2}{||w||} \\ s.t. y_i(w^tx_i + b) \geq 1,i=1,2,...,N$$

- 等价的对偶问题：$$\min\limits_{w,b} \frac{1}{2}||w||^2 \\ s.t. y_i(w^tx_i + b) \geq 1,i=1,2,...,N$$

### 对偶法 -- 模型参数学习

引入拉格朗日乘子，建立朗格朗日函数：$$L(w,b,\alpha) = \frac{1}{2}||w||^2 + \sum_{n = 1}^N\alpha_i(1 - y_i(w^Tx_i + b)) \\ = \frac{1}{2}w^Tw + \sum_{n = 1}^N\alpha_i - \sum_{n = 1}^N\alpha_iy_i(w^Tx_i +b)$$ ，其中 $\alpha_i$ 为拉格朗日乘子

==》等价于求 $$\max \limits_{\alpha} \min\limits_{w,b} L(w,b,\alpha)$$

1. 求解 $\min \limits_{w,b}L(w,b,\alpha)$

    分别对 $w,b$ 求偏导，并求出偏导为零的值

    $$\frac{\partial L}{\partial w} = w - \sum_{i = 1}^{N}\alpha_iy_ix_i = 0 \Longrightarrow w = \sum_{i = 1}^N\alpha_iy_ix_i$$

    $$\frac{\partial L}{\partial b} = -\sum_{i = 1}^N\alpha_iy_i = 0 \Longrightarrow \sum_{i = 1}^N\alpha_iy_i = 0 $$

    代入拉格朗日函数得：

    

    <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI202210302259736.png" alt="iShot_2022-10-30_22.52.31" style="zoom:50%;" />

2. 对 $\alpha$ 求极大 

    <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI202210302259092.png" alt="iShot_2022-10-30_22.52.40" style="zoom:50%;" />

SMO 算法

## 软间隔最大化

## 非线性支持向量机与核函数

## SVM 优缺点

任何算法都有其优缺点，支持向量机也不例外。

支持向量机的优点是:

> 1. 由于SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。
> 2. 不仅适用于线性线性问题还适用于非线性问题(用核技巧)。
> 3. 拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，这在某种意义上避免了“维数灾难”。
> 4. 理论基础比较完善(例如神经网络就更像一个黑盒子)。

支持向量机的缺点是:

> 1. 二次规划问题求解将涉及m阶矩阵的计算(m为样本的个数), 因此SVM不适用于超大数据集。(SMO算法可以缓解这个问题)
> 2. 只适用于二分类问题。(SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题)

















