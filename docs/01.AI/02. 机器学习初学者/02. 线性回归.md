---
title: 线性回归
date: 2022-10-15 18:49:16
permalink: /pages/036368/
tags: 
  - 
---
# 线性回归

## 线性回归算法简介

线性回归是通过属性的线性组合来进行预测的函数，即

::: center

​	$f(x) = w_1x_1 + w_2x_2 + ...+ w_nx_n + b$ ，使得 $f(x_i) \simeq y_i$

:::

用向量形式表示为

​	$f(x) = w^Tx + b$

::: tip

离散属性的处理：若有序，则连续化；否则，转化为 K 维向量

:::

## 参数估计--最小二乘法

**确定w和b是最关键的**,我们采用均方误差来衡量算法的性能，所以我们的目的就是让线性回归模型的均方误差最小化，数学公式表达为:

​	$\hat {w^*} = \mathop {\arg\min}\limits_{\hat{w}}\sum_{i = 1}^{n} (f(x_i) - y_i)^2$

即

​	$\hat {w^*} =\mathop {\arg\min}\limits_{\hat{w}}(y - X \hat{w})^T(y - X \hat{w})$

对 $\hat{w}$ 求导

<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI/202210151953771.png" alt="iShot_2022-10-15_19.51.57" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI/202210152002637.png" alt="iShot_2022-10-15_19.52.06" style="zoom:50%;" />

令其等于零，可求解 $\hat{w}$ :

​	==$\hat{w^*} = (X^TX)^{-1}X^Ty$==

- 若 $X^TX$ 不可逆，则可解出多个 $\hat{w^*}$ ；此时可以采用引入正则项来解决

<u>对于一元线性回归，有</u>

<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI/202210151932989.png" alt="iShot_2022-10-15_19.30.40" style="zoom: 25%;" />

## 广义线性模型

对数线性回归的公式如下：

​	$\ln{y} = w^Tx + b$

它实际上是在试图让 $e^{w^Tx + b}$ 逼近 $y$ .形式上仍是线性回归，但实质上**在求取输入空间到输出空间的非线性函数映射**，更一般地，考虑单调可微函数 $g(x)$ ，令

​	$y = g^{-1}(w^Tx + b)$

这样的模型称作**广义线性模型**，其中函数g(x)被成为联系函数。显然当联系函数g(x)=ln(x)时，便是对数线性回归。



