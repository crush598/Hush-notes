---
title: Logistic 回归
date: 2022-10-15 20:21:16
permalink: /pages/ba793c/
tags: 
  - 
---
#  Logistic 回归

## Logistic 回归算法简介

逻辑回归是机器学习中的经典分类问题，属于对数线性模型，又称对数几率回归。Logistic 回归可以看作预测值为 <mark>“标签的对数几率”</mark> 的线性回归模型

::: note 对数几率函数

​	一种 “Sigmoid 函数”，优点单调可微，任意阶可导。

:::

::: center

​	$y = \frac{1}{1 + e^{-z}}$ 即 $\ln{\frac{y}{1-y}} = w^TX + b$ 反映了 x 作为正例的相对可能性

:::

## 模型参数估计 -- 极大似然法

### 损失函数

损失函数的定义：使 $\hat{y}$ 值和数据的标签尽可能接近

最终转化为最小化：

::: center

​	$loss = \sum_{n = 1}^m (-y_i(w^TX_i) + \ln{(1 + e^{w^TX_i})})$

:::

::: details

​	具体推导过程如下:

​	<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI/202210161617092.jpg" alt="未命名10" style="zoom:50%;" />

:::

### 梯度下降法

准备工作

::: details

$z = w_0 + w_1x_1 + w_2x_2 = \begin {bmatrix} 1 & x_1 & x_2\end {bmatrix}\begin {bmatrix} w_0\\ w_1 \\ w_2\end {bmatrix}$

$Z = XW ; \frac{\partial Z}{\partial W} = X^T$

$prob = \frac{1}{1 + e^{-z}} ; \frac{\mathrm{d}prob}{\mathrm{d}z} = \frac{e^{-z}}{(1 + e^{-z})^2 } = \frac{1}{1 + e^{-z}}\frac{e^{-z}}{1 + e^{-z}} = prob(1 - prob)$

$loss = -y\ln(prob) - (1-y)ln(1-prob); \frac{\mathrm{d}loss}{\mathrm{d}prob} = -\frac{y}{prob} + \frac{1 - y}{1-prob}$

:::

:::center

<mark>梯度计算公式</mark>

$grad = \frac{\mathrm{d}loss}{\mathrm{d}W} = \frac{\mathrm{d}loss}{\mathrm{d}prob} * \frac{\mathrm{d}prob}{\mathrm{d}z} * \frac{\mathrm{d}z}{\mathrm{d}W}$

$=&X^T(-\frac{y}{prob} +\frac{1 - y}{1-prob})*prob(1 - prob) \\ =& X^T(-y(1-prob) + (1-y)prob) \\ =&X^T(prob - y)$

:::

**求解步骤**

步骤1. 初始化W

步骤2. W -= lr * grad (其中lr为学习率，值到0-1之间)

步骤3. 迭代都一定次数，或阈值停止

### 牛顿法









