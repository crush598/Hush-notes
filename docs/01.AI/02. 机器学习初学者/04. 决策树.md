---
title:  决策树
date: 2022-10-16 21:02:05
permalink: /pages/4ed12f/
tags:
  - 
---
# 决策树

## 决策树简介

决策树（Decision tree）是基于已知各种情况（特征取值）的基础上，通过构建树型决策结构来进行分析的一种方式，是常用的有监督的分类算法。

决策树学习通常包括 3 个步骤：特征选择、决策树的生成和决策树的修剪

<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI/202210181045434.png" alt="决策树的发展史" style="zoom:50%;" />

## 特征选择

决策树的关键在于：**选取最优划分属性**。一般而言，我们希望随着决策树的划分过程，**决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度越来越高**。

<u>**通常特征选择的准则是信息增益、信息增益率或基尼指数**</u>

::: details

### （1）信息熵

- 信息熵（information entropy）：度量样本纯度，**信息熵的值越小，D 的纯度越高**。假定当前样本集合 D 中第 k 类样本的比例为 $p_K$，一共有 Y 个类别，则信息熵定义为：
- $Ent(D) = - \sum_{k = 1}^{Y}p_k\log_2p_k$

### （2）信息增益

- 信息增益（Information Gain）：衡量的是我们**选择某个属性进行划分时信息熵的变化**（可以理解为基于这个规则划分，不确定性降低的程度）。一般而言，信息增益越大，说明选用该属性对划分的”纯度提升“越大，分支效果越好。
- $Gain(D,a) = Ent(D) - \sum_{v = 1}^{V} \frac{|D^v|}{|D|}Ent(D^v)$
- <u>信息增益准则对可取值数目较多的属性有所偏好(即偏好分支多的属性)</u>
- <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI/202210181110873.png" alt="iShot_2022-10-18_11.10.05" style="zoom:50%;" />

### （3）信息增益率

- 信息增益率（Gain Ratio）:信息增益率相比信息增益，多了一个衡量本身属性的分散程度的部分作为分母。<u>信息增益率对可取值数目较少的属性有所偏好</u>
- $Gain-ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$
- $IV(a) = -\sum_{v = 1}^{V} \frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}$
- <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI/202210181120343.png" alt="iShot_2022-10-18_11.20.00" style="zoom:50%;" />

### （4）基尼指数

- **基尼指数**（Gini Index）:基尼指数表示从数据集D中**随机抽取两个样本，其类别标记不一致的的概率**。因此，概率越小，即**基尼指数越小，D 纯度越高**。
- $Gini(D) = \sum_{k = 1}^{Y}\sum_{k^`\ne k}p_kp_{k^`} = 1 - \sum_{k = 1}^{Y}p_k^2$
- $Gini-index(D,a) = \sum_{v = 1}^{V}\frac{|D^v|}{|D|}Gini(D^v)$
- <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI/202210181123311.png" alt="iShot_2022-10-18_11.23.03" style="zoom:50%;" />

:::

## 决策树的生成

**决策树的总体流程是自根至叶的递归过程，在每个中间结点寻找一个「划分」（split or test）属性**。

- 算法基本流程

<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/AI/202210181048207.png" alt="iShot_2022-10-18_10.48.16" style="zoom:50%;" />

- 决策树停止生长的三个条件：
    1. 当前结点包含的样本全属于同一类别
    2. 样本的属性取值都相同或属性集为空，无法划分
    3. 当前结点包含样本集合为空

## 决策树的剪枝

基本策略包含「**预剪枝**」和「**后剪枝**」两个：

**预剪枝**（**pre-pruning**）：在决策树生长过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶结点。

- 优点：使得决策树的很多分支都没有展开，**不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。**
- 缺点：有些分支的当前划分虽然不能提升泛化性能，甚至导致泛化性能下降，但**在其基础上进行的后续划分可能导致性能显著提升**。预剪枝基于**贪心**的思想，本质上禁止这些分支展开，给预剪枝决策树带来了**欠拟合**的风险

**后剪枝**（**post-pruning**）：先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能的提升，则将该子树替换为叶结点。

- 优点：可以看出，**后剪枝决策树比预剪枝决策树保留了更多的分支**。一般情况中，后剪枝决策树**欠拟合的风险比较小，泛化性能也往往优于预剪枝决策树**。
- 缺点：后剪枝过程是完整生成决策树之后再进行的，并且要自底向上，对所有非叶节点注意观察，**训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。**

## 连续与缺失值

对于连续属性：由于连续属性的可取值数目不再是有限的，因此可以采用**连续属性离散化技术**。

对于缺失值：方法--“<mark>样本赋权，权重划分</mark>”



参考资料 ： 

- https://juejin.cn/post/7070496988256534558
- https://www.showmeai.tech/article-detail/190





