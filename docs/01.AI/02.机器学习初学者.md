---
title: 机器学习初学者
date: 2022-08-21 23:53:25
permalink: /pages/c08091/
---
## 简介

- AI、ML、深度学习和数据科学之间关系的图表![AI、ML、深度学习和数据科学之间关系](https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208191645653.png)

- 监督学习
- 无监督学习
- 特征选取--》过拟合

## 机器学习的历史

## 机器学习的流程

1. 收集整理数据----**特征选择和特征提取**
2. 可视化数据、拆分数据集（训练集、测试集）
3. **建立模型----训练模型、评估模型**
4. 参数调优

## 线性回归

- 模型和损失函数

    - ![线性回归的简述](https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208211500278.png)

- 梯度下降

    - <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208222056094.png" alt="一元与多元线性回归的梯度下降" style="zoom:50%;" />

- **多元线性回归**

    > - <u>向量的点积</u>
    > - <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208231905636.png" alt="向量的点积" style="zoom:40%;" />
    >

## 分类

### Logistic 回归

- Sigmoid function<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252119737.png" alt="Sigmoid function" style="zoom:50%;" />

- 决策边界 decision boundary![decision boundary](https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252337873.jpg)

- 损失函数 Loss Function <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252334800.png" alt="Loss Function" style="zoom:50%;" />

- <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252343175.jpg" alt="损失函数" style="zoom:50%;" />

- 梯度下降<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208281349036.png" alt="梯度下降" style="zoom:50%;" />

    尽管梯度下降的方法在线性回归和 Logistic 回归上的表现形式一样；当是两者的函数的定义是不同的。

## 神经网络 Neural networks

### 感知机（前身）

### 神经网络的简介

- ![iShot_2022-08-29_22.08.54](https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208292210256.png)

## **tips**

- 数据预处理

    > [特征缩放](https://www.cnblogs.com/HuZihu/p/9761161.html)
    >
    > - **最大最小值归一化（min-max normalization）**：将数值范围缩放到 [0, 1] 区间里
    >
    > - **均值归一化（mean normalization）**：将数值范围缩放到 [-1, 1] 区间里，且数据的均值变为0
    > - **标准化 / z值归一化（\**s\*\*tandardization /\*\**\* z-score \**normalization\**\**）\****：将数值缩放到0附近，且数据的分布变为均值为0，标准差为1的标准正态分布（先减去均值来对特征进行 **中心化 mean centering** 处理，再除以标准差进行缩放）

    - 通过绘制学习曲线来帮助判断梯度下降是否收敛--学习率的选择

    > [特征工程](https://www.cnblogs.com/peizhe123/p/7412364.html)
    >
    > - 

- 过拟合 Overfitting 

    > - Bias
    > - Variance
    > - 解决方法--过拟合<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208281416273.png" alt="解决方法--过拟合" style="zoom:50%;" />
    >     1. 收集更多的训练数据
    >     2. 减少使用的特征（feature）的数量--即挑选更有用的特征
    >     3. **<u>正则化</u>**

- 正则化



























