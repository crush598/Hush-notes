---
title: 机器学习初学者
date: 2022-08-21 23:53:25
permalink: /pages/c08091/
---
## 简介

- AI、ML、深度学习和数据科学之间关系的图表![AI、ML、深度学习和数据科学之间关系](https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208191645653.png)

- 监督学习
- 无监督学习
- 特征选取--》过拟合

## 机器学习的历史

## 机器学习的流程

1. 收集整理数据----**特征选择和特征提取**
2. 可视化数据、拆分数据集（训练集、测试集）
3. **建立模型----训练模型、评估模型**
4. 参数调优<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202209192123267.png" alt="iShot_2022-09-19_21.01.29" style="zoom:50%;" />

## 线性回归

- 模型和损失函数

    - ![线性回归的简述](https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208211500278.png)

### 梯度下降

- <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208222056094.png" alt="一元与多元线性回归的梯度下降" style="zoom:50%;" />

- #### [梯度下降法的矩阵方式描述](https://wangcongying.com/2019/09/26/summaryOfGradientDescent/#toc-heading-43)(代码实现比较简洁)

    - ### 批量梯度下降法（Batch Gradient Descent, BGD）

    - ### 随机梯度下降法（Stochastic Gradient Descent, SGD）

    - ### 小批量梯度下降法（Mini-batch Gradient Descent, MBGD）

- **多元线性回归**


> - 向量的点积</u>
> - <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208231905636.png" alt="向量的点积" style="zoom:40%;" />
>

### 高阶优化方法

- Adam Algorithm

## 分类

### Logistic 回归

- Sigmoid function<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252119737.png" alt="Sigmoid function" style="zoom:50%;" />

- 决策边界 decision boundary![decision boundary](https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252337873.jpg)

- 损失函数 Loss Function <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252334800.png" alt="Loss Function" style="zoom:50%;" />

- <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252343175.jpg" alt="损失函数" style="zoom:50%;" />

- 梯度下降<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208281349036.png" alt="梯度下降" style="zoom:50%;" />

    尽管梯度下降的方法在线性回归和 Logistic 回归上的表现形式一样；当是两者的函数的定义是不同的。

## 神经网络 Neural networks

### 感知机（前身）

### 神经网络的简介

- ![iShot_2022-08-29_22.08.54](https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208292210256.png)
- **正向传播**
- 激活函数<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202209130940988.png" alt="激活函数" style="zoom:50%;" />
- **Softmax 函数**
- 隐藏层
- **卷积层----仅计算部分输入**
- 输出层
- **反向传播**

### 如何提升算法的性能

- 划分训练集和测试集
- 交叉验证
- 建立表现基准
- 绘制学习曲线

## 决策树

- 

## **tips**

- 数据预处理

    > [特征缩放](https://www.cnblogs.com/HuZihu/p/9761161.html)
    >
    > - **最大最小值归一化（min-max normalization）**：将数值范围缩放到 [0, 1] 区间里
    >
    > - **均值归一化（mean normalization）**：将数值范围缩放到 [-1, 1] 区间里，且数据的均值变为0
    > - **标准化 / z值归一化（\**s\*\*tandardization /\*\**\* z-score \**normalization\**\**）\****：将数值缩放到0附近，且数据的分布变为均值为0，标准差为1的标准正态分布（先减去均值来对特征进行 **中心化 mean centering** 处理，再除以标准差进行缩放）

    - 通过绘制学习曲线来帮助判断梯度下降是否收敛--学习率的选择

    > [特征工程](https://www.cnblogs.com/peizhe123/p/7412364.html)
    
- 过拟合 Overfitting <img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202209130927330.png" alt="iShot_2022-09-13_09.27.11" style="zoom:50%;" />

    > - Bias -- 模型在训练集上的误差
    > - Variance -- 训练集误差和测试集误差之间的差距
    > - 解决方法--过拟合<img src="https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208281416273.png" alt="解决方法--过拟合" style="zoom:50%;" />
    >     1. 收集更多的训练数据
    >     2. 减少使用的特征（feature）的数量--即挑选更有用的特征
    >     3. **<u>正则化</u>**

- [正则化](https://lavi-liu.blog.csdn.net/article/details/99984288)

    - 参考内容：https://lulaoshi.info/machine-learning/linear-model/regularization



























