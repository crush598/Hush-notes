(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{409:function(t,a,s){"use strict";s.r(a);var r=s(0),i=Object(r.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#简介"}},[t._v("#")]),t._v(" 简介")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("AI、ML、深度学习和数据科学之间关系的图表"),a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208191645653.png",alt:"AI、ML、深度学习和数据科学之间关系"}})])]),t._v(" "),a("li",[a("p",[t._v("监督学习")])]),t._v(" "),a("li",[a("p",[t._v("无监督学习")])]),t._v(" "),a("li",[a("p",[t._v("特征选取--》过拟合")])])]),t._v(" "),a("h2",{attrs:{id:"机器学习的历史"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习的历史"}},[t._v("#")]),t._v(" 机器学习的历史")]),t._v(" "),a("h2",{attrs:{id:"机器学习的流程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习的流程"}},[t._v("#")]),t._v(" 机器学习的流程")]),t._v(" "),a("ol",[a("li",[t._v("收集整理数据----"),a("strong",[t._v("特征选择和特征提取")])]),t._v(" "),a("li",[t._v("可视化数据、拆分数据集（训练集、测试集）")]),t._v(" "),a("li",[a("strong",[t._v("建立模型----训练模型、评估模型")])]),t._v(" "),a("li",[t._v("参数调优")])]),t._v(" "),a("h2",{attrs:{id:"线性回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#线性回归"}},[t._v("#")]),t._v(" 线性回归")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("模型和损失函数")]),t._v(" "),a("ul",[a("li",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208211500278.png",alt:"线性回归的简述"}})])])])]),t._v(" "),a("h3",{attrs:{id:"梯度下降"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#梯度下降"}},[t._v("#")]),t._v(" 梯度下降")]),t._v(" "),a("ul",[a("li",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208222056094.png",alt:"一元与多元线性回归的梯度下降"}})]),t._v(" "),a("li",[a("h4",{attrs:{id:"梯度下降法的矩阵方式描述-代码实现比较简洁"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#梯度下降法的矩阵方式描述-代码实现比较简洁"}},[t._v("#")]),t._v(" "),a("a",{attrs:{href:"https://wangcongying.com/2019/09/26/summaryOfGradientDescent/#toc-heading-43",target:"_blank",rel:"noopener noreferrer"}},[t._v("梯度下降法的矩阵方式描述"),a("OutboundLink")],1),t._v("(代码实现比较简洁)")]),t._v(" "),a("ul",[a("li",[a("h3",{attrs:{id:"批量梯度下降法-batch-gradient-descent-bgd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#批量梯度下降法-batch-gradient-descent-bgd"}},[t._v("#")]),t._v(" 批量梯度下降法（Batch Gradient Descent, BGD）")])]),t._v(" "),a("li",[a("h3",{attrs:{id:"随机梯度下降法-stochastic-gradient-descent-sgd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#随机梯度下降法-stochastic-gradient-descent-sgd"}},[t._v("#")]),t._v(" 随机梯度下降法（Stochastic Gradient Descent, SGD）")])]),t._v(" "),a("li",[a("h3",{attrs:{id:"小批量梯度下降法-mini-batch-gradient-descent-mbgd"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#小批量梯度下降法-mini-batch-gradient-descent-mbgd"}},[t._v("#")]),t._v(" 小批量梯度下降法（Mini-batch Gradient Descent, MBGD）")])])])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("多元线性回归")])])])]),t._v(" "),a("blockquote",[a("ul",[a("li",[t._v("向量的点积")]),t._v(" "),a("li",[a("img",{staticStyle:{zoom:"40%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208231905636.png",alt:"向量的点积"}})])])]),t._v(" "),a("h3",{attrs:{id:"高阶优化方法"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#高阶优化方法"}},[t._v("#")]),t._v(" 高阶优化方法")]),t._v(" "),a("ul",[a("li",[t._v("Adam Algorithm")])]),t._v(" "),a("h2",{attrs:{id:"分类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分类"}},[t._v("#")]),t._v(" 分类")]),t._v(" "),a("h3",{attrs:{id:"logistic-回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#logistic-回归"}},[t._v("#")]),t._v(" Logistic 回归")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("Sigmoid function"),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252119737.png",alt:"Sigmoid function"}})])]),t._v(" "),a("li",[a("p",[t._v("决策边界 decision boundary"),a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208252337873.jpg",alt:"decision boundary"}})])]),t._v(" "),a("li",[a("p",[t._v("损失函数 Loss Function "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252334800.png",alt:"Loss Function"}})])]),t._v(" "),a("li",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252343175.jpg",alt:"损失函数"}})]),t._v(" "),a("li",[a("p",[t._v("梯度下降"),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208281349036.png",alt:"梯度下降"}})]),t._v(" "),a("p",[t._v("尽管梯度下降的方法在线性回归和 Logistic 回归上的表现形式一样；当是两者的函数的定义是不同的。")])])]),t._v(" "),a("h2",{attrs:{id:"神经网络-neural-networks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#神经网络-neural-networks"}},[t._v("#")]),t._v(" 神经网络 Neural networks")]),t._v(" "),a("h3",{attrs:{id:"感知机-前身"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#感知机-前身"}},[t._v("#")]),t._v(" 感知机（前身）")]),t._v(" "),a("h3",{attrs:{id:"神经网络的简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#神经网络的简介"}},[t._v("#")]),t._v(" 神经网络的简介")]),t._v(" "),a("ul",[a("li",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208292210256.png",alt:"iShot_2022-08-29_22.08.54"}})]),t._v(" "),a("li",[a("strong",[t._v("正向传播")])]),t._v(" "),a("li",[t._v("激活函数"),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202209130940988.png",alt:"激活函数"}})]),t._v(" "),a("li",[a("strong",[t._v("Softmax 函数")])]),t._v(" "),a("li",[t._v("隐藏层")]),t._v(" "),a("li",[t._v("卷积层----仅计算部分输入")]),t._v(" "),a("li",[t._v("输出层")]),t._v(" "),a("li",[a("strong",[t._v("反向传播")])])]),t._v(" "),a("h3",{attrs:{id:"如何提升算法的性能"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#如何提升算法的性能"}},[t._v("#")]),t._v(" 如何提升算法的性能")]),t._v(" "),a("ul",[a("li")]),t._v(" "),a("h2",{attrs:{id:"决策树"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#决策树"}},[t._v("#")]),t._v(" 决策树")]),t._v(" "),a("ul",[a("li")]),t._v(" "),a("h2",{attrs:{id:"tips"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tips"}},[t._v("#")]),t._v(" "),a("strong",[t._v("tips")])]),t._v(" "),a("ul",[a("li",[a("p",[t._v("数据预处理")]),t._v(" "),a("blockquote",[a("p",[a("a",{attrs:{href:"https://www.cnblogs.com/HuZihu/p/9761161.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("特征缩放"),a("OutboundLink")],1)]),t._v(" "),a("ul",[a("li",[a("p",[a("strong",[t._v("最大最小值归一化（min-max normalization）")]),t._v("：将数值范围缩放到 [0, 1] 区间里")])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("均值归一化（mean normalization）")]),t._v("：将数值范围缩放到 [-1, 1] 区间里，且数据的均值变为0")])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("标准化 / z值归一化（*"),a("em",[t._v("s**tandardization /**")]),t._v("* z-score *"),a("em",[t._v("normalization*")]),t._v("*"),a("em",[t._v("）*")])]),t._v("：将数值缩放到0附近，且数据的分布变为均值为0，标准差为1的标准正态分布（先减去均值来对特征进行 "),a("strong",[t._v("中心化 mean centering")]),t._v(" 处理，再除以标准差进行缩放）")])])])]),t._v(" "),a("ul",[a("li",[t._v("通过绘制学习曲线来帮助判断梯度下降是否收敛--学习率的选择")])]),t._v(" "),a("blockquote",[a("p",[a("a",{attrs:{href:"https://www.cnblogs.com/peizhe123/p/7412364.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("特征工程"),a("OutboundLink")],1)])])]),t._v(" "),a("li",[a("p",[t._v("过拟合 Overfitting "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202209130927330.png",alt:"iShot_2022-09-13_09.27.11"}})]),t._v(" "),a("blockquote",[a("ul",[a("li",[t._v("Bias -- 模型在训练集上的误差")]),t._v(" "),a("li",[t._v("Variance -- 训练集误差和测试集误差之间的差距")]),t._v(" "),a("li",[t._v("解决方法--过拟合"),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208281416273.png",alt:"解决方法--过拟合"}}),t._v(" "),a("ol",[a("li",[t._v("收集更多的训练数据")]),t._v(" "),a("li",[t._v("减少使用的特征（feature）的数量--即挑选更有用的特征")]),t._v(" "),a("li",[a("strong",[a("u",[t._v("正则化")])])])])])])])]),t._v(" "),a("li",[a("p",[a("a",{attrs:{href:"https://lavi-liu.blog.csdn.net/article/details/99984288",target:"_blank",rel:"noopener noreferrer"}},[t._v("正则化"),a("OutboundLink")],1)]),t._v(" "),a("ul",[a("li",[t._v("参考内容：https://lulaoshi.info/machine-learning/linear-model/regularization")])])])])])}),[],!1,null,null,null);a.default=i.exports}}]);