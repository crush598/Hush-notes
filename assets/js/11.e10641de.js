(window.webpackJsonp=window.webpackJsonp||[]).push([[11],{405:function(t,a,s){"use strict";s.r(a);var i=s(0),r=Object(i.a)({},(function(){var t=this,a=t._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#简介"}},[t._v("#")]),t._v(" 简介")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("AI、ML、深度学习和数据科学之间关系的图表"),a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208191645653.png",alt:"AI、ML、深度学习和数据科学之间关系"}})])]),t._v(" "),a("li",[a("p",[t._v("监督学习")])]),t._v(" "),a("li",[a("p",[t._v("无监督学习")])]),t._v(" "),a("li",[a("p",[t._v("特征选取--》过拟合")])])]),t._v(" "),a("h2",{attrs:{id:"机器学习的历史"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习的历史"}},[t._v("#")]),t._v(" 机器学习的历史")]),t._v(" "),a("h2",{attrs:{id:"机器学习的流程"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#机器学习的流程"}},[t._v("#")]),t._v(" 机器学习的流程")]),t._v(" "),a("ol",[a("li",[t._v("收集整理数据----"),a("strong",[t._v("特征选择和特征提取")])]),t._v(" "),a("li",[t._v("可视化数据、拆分数据集（训练集、测试集）")]),t._v(" "),a("li",[a("strong",[t._v("建立模型----训练模型、评估模型")])]),t._v(" "),a("li",[t._v("参数调优")])]),t._v(" "),a("h2",{attrs:{id:"线性回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#线性回归"}},[t._v("#")]),t._v(" 线性回归")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("模型和损失函数")]),t._v(" "),a("ul",[a("li",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208211500278.png",alt:"线性回归的简述"}})])])]),t._v(" "),a("li",[a("p",[t._v("梯度下降")]),t._v(" "),a("ul",[a("li",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208222056094.png",alt:"一元与多元线性回归的梯度下降"}})])])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("多元线性回归")])]),t._v(" "),a("blockquote",[a("ul",[a("li",[a("u",[t._v("向量的点积")])]),t._v(" "),a("li",[a("img",{staticStyle:{zoom:"40%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208231905636.png",alt:"向量的点积"}})])])])])]),t._v(" "),a("h2",{attrs:{id:"分类"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#分类"}},[t._v("#")]),t._v(" 分类")]),t._v(" "),a("h3",{attrs:{id:"logistic-回归"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#logistic-回归"}},[t._v("#")]),t._v(" Logistic 回归")]),t._v(" "),a("ul",[a("li",[a("p",[t._v("Sigmoid function"),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252119737.png",alt:"Sigmoid function"}})])]),t._v(" "),a("li",[a("p",[t._v("决策边界 decision boundary"),a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208252337873.jpg",alt:"decision boundary"}})])]),t._v(" "),a("li",[a("p",[t._v("损失函数 Loss Function "),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252334800.png",alt:"Loss Function"}})])]),t._v(" "),a("li",[a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208252343175.jpg",alt:"损失函数"}})]),t._v(" "),a("li",[a("p",[t._v("梯度下降"),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208281349036.png",alt:"梯度下降"}})]),t._v(" "),a("p",[t._v("尽管梯度下降的方法在线性回归和 Logistic 回归上的表现形式一样；当是两者的函数的定义是不同的。")])])]),t._v(" "),a("h2",{attrs:{id:"神经网络-neural-networks"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#神经网络-neural-networks"}},[t._v("#")]),t._v(" 神经网络 Neural networks")]),t._v(" "),a("h3",{attrs:{id:"感知机-前身"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#感知机-前身"}},[t._v("#")]),t._v(" 感知机（前身）")]),t._v(" "),a("h3",{attrs:{id:"神经网络的简介"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#神经网络的简介"}},[t._v("#")]),t._v(" 神经网络的简介")]),t._v(" "),a("ul",[a("li",[a("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/二分/202208292210256.png",alt:"iShot_2022-08-29_22.08.54"}})]),t._v(" "),a("li",[a("strong",[t._v("正向传播")])]),t._v(" "),a("li",[a("strong",[t._v("反向传播")])])]),t._v(" "),a("h2",{attrs:{id:"tips"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#tips"}},[t._v("#")]),t._v(" "),a("strong",[t._v("tips")])]),t._v(" "),a("ul",[a("li",[a("p",[t._v("数据预处理")]),t._v(" "),a("blockquote",[a("p",[a("a",{attrs:{href:"https://www.cnblogs.com/HuZihu/p/9761161.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("特征缩放"),a("OutboundLink")],1)]),t._v(" "),a("ul",[a("li",[a("p",[a("strong",[t._v("最大最小值归一化（min-max normalization）")]),t._v("：将数值范围缩放到 [0, 1] 区间里")])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("均值归一化（mean normalization）")]),t._v("：将数值范围缩放到 [-1, 1] 区间里，且数据的均值变为0")])]),t._v(" "),a("li",[a("p",[a("strong",[t._v("标准化 / z值归一化（*"),a("em",[t._v("s**tandardization /**")]),t._v("* z-score *"),a("em",[t._v("normalization*")]),t._v("*"),a("em",[t._v("）*")])]),t._v("：将数值缩放到0附近，且数据的分布变为均值为0，标准差为1的标准正态分布（先减去均值来对特征进行 "),a("strong",[t._v("中心化 mean centering")]),t._v(" 处理，再除以标准差进行缩放）")])])])]),t._v(" "),a("ul",[a("li",[t._v("通过绘制学习曲线来帮助判断梯度下降是否收敛--学习率的选择")])]),t._v(" "),a("blockquote",[a("p",[a("a",{attrs:{href:"https://www.cnblogs.com/peizhe123/p/7412364.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("特征工程"),a("OutboundLink")],1)]),t._v(" "),a("ul",[a("li")])])]),t._v(" "),a("li",[a("p",[t._v("过拟合 Overfitting")]),t._v(" "),a("blockquote",[a("ul",[a("li",[t._v("Bias")]),t._v(" "),a("li",[t._v("Variance")]),t._v(" "),a("li",[t._v("解决方法--过拟合"),a("img",{staticStyle:{zoom:"50%"},attrs:{src:"https://cdn.jsdelivr.net/gh/crush598/image@main/%E4%BA%8C%E5%88%86/202208281416273.png",alt:"解决方法--过拟合"}}),t._v(" "),a("ol",[a("li",[t._v("收集更多的训练数据")]),t._v(" "),a("li",[t._v("减少使用的特征（feature）的数量--即挑选更有用的特征")]),t._v(" "),a("li",[a("strong",[a("u",[t._v("正则化")])])])])])])])]),t._v(" "),a("li",[a("p",[a("a",{attrs:{href:"https://lavi-liu.blog.csdn.net/article/details/99984288",target:"_blank",rel:"noopener noreferrer"}},[t._v("正则化"),a("OutboundLink")],1)])])])])}),[],!1,null,null,null);a.default=r.exports}}]);